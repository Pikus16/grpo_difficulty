{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 06-29 18:20:48 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 06-29 18:20:48 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import textstat\n",
    "from vllm import SamplingParams\n",
    "import os\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "import click\n",
    "import re\n",
    "from datasets import Dataset\n",
    "import wandb\n",
    "import numpy as np\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ---------- Constants ----------\n",
    "PROMPT = \"\"\"Solve the following math word problem.\n",
    "\n",
    "{q}\n",
    "\n",
    "Think step-by-step. Then, provide the final answer as a single integer in the format \"Answer: XXX\" with no extra formatting.\"\"\"\n",
    "\n",
    "\n",
    "# ---------- Utility Functions ----------\n",
    "def make_dataset(difficulty_level, dir_path='outputs/gsm8k_platinum/accuracy_subset', subset='train'):\n",
    "    ds = load_dataset(\"json\", data_files=f'{dir_path}/{difficulty_level}_{subset}.jsonl', split=\"train\")\n",
    "    def format_prompt(example):\n",
    "        new_prompt = PROMPT.format(q=example['question'])\n",
    "        return {'prompt' : new_prompt}\n",
    "    ds = ds.map(format_prompt)\n",
    "\n",
    "    ds = ds.map(lambda x: {\"answer\": x[\"parsed\"]})\n",
    "    ds = ds.remove_columns(\"parsed\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, adapter=None):\n",
    "    max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "    dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "    model_, tokenizer_ = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_name,\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    )\n",
    "    if adapter is not None:\n",
    "        model_.load_adapter(adapter)\n",
    "    _ = FastLanguageModel.for_inference(model_) # Enable native 2x faster inference\n",
    "    return model_, tokenizer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_batch(tokenizer, model, prompts, num_times_to_repeat: int = 8, apply_template: bool = False):\n",
    "    \"\"\"\n",
    "    Process multiple prompts in a single batch for better GPU utilization\n",
    "    \"\"\"\n",
    "    generation_kwargs = {\n",
    "        \"max_new_tokens\": 250,\n",
    "        \"use_cache\": True,\n",
    "        \"temperature\": 0.9,\n",
    "        \"top_k\": None,\n",
    "        \"do_sample\": True,\n",
    "    }\n",
    "\n",
    "    # Create all formatted prompts at once\n",
    "    if apply_template:\n",
    "        all_formatted_prompts = []\n",
    "        for prompt in prompts:\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                [{'role': 'user', 'content': prompt}],\n",
    "                tokenize=False, add_generation_prompt=True)\n",
    "            all_formatted_prompts.extend([formatted_prompt] * num_times_to_repeat)\n",
    "    else:\n",
    "        all_formatted_prompts = prompts * num_times_to_repeat\n",
    "    \n",
    "    # Tokenize in larger batches\n",
    "    inputs = tokenizer(all_formatted_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        outputs = model.generate(**inputs, **generation_kwargs)\n",
    "    \n",
    "    outputs = outputs[:, inputs.input_ids.shape[1]:]\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "    # Reshape outputs back to per-prompt format\n",
    "    result = []\n",
    "    for i in range(len(prompts)):\n",
    "        start_idx = i * num_times_to_repeat\n",
    "        end_idx = start_idx + num_times_to_repeat\n",
    "        result.append(decoded_outputs[start_idx:end_idx])\n",
    "    \n",
    "    return result\n",
    "\n",
    "ANSWER_PATTERN = re.compile(r\"Answer:\\s*(-?\\d+)\")\n",
    "def parse_llm_answer(text):\n",
    "    \"\"\"\n",
    "    Extracts the final answer from the LLM output.\n",
    "    Expects the format: \"Answer: XXX\" where XXX is an integer.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The output from the LLM.\n",
    "    \n",
    "    Returns:\n",
    "        float or None: The extracted float answer, or None if not found.\n",
    "    \"\"\"\n",
    "    match = ANSWER_PATTERN.search(text)\n",
    "    try:\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    except:\n",
    "        return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.8: Fast Qwen3 patching. Transformers: 4.52.4. vLLM: 0.8.5.\n",
      "   \\\\   /|    NVIDIA A10G. Num GPUs = 1. Max memory: 22.069 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model('unsloth/Qwen3-4B-unsloth-bnb-4bit', 'models/gsm8k/difficulty37_8gen_1k_qwen4b/lora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = make_dataset(37, subset='test')\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/run/lib/python3.10/site-packages/unsloth/kernels/utils.py:655: UserWarning: An output with one or more elements was resized since it had shape [1, 4, 2560], which does not match the required output shape [4, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n",
      "/home/ubuntu/miniconda3/envs/run/lib/python3.10/site-packages/unsloth/kernels/utils.py:660: UserWarning: An output with one or more elements was resized since it had shape [1, 4, 2560], which does not match the required output shape [4, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n",
      "  out = torch_matmul(X, W, out = out)\n"
     ]
    }
   ],
   "source": [
    "prompts = [x['prompt'] for x in ds][:4]\n",
    "outputs = get_answer_batch(tokenizer, model, prompts, num_times_to_repeat=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/run/lib/python3.10/site-packages/unsloth/kernels/utils.py:655: UserWarning: An output with one or more elements was resized since it had shape [1, 4, 2560], which does not match the required output shape [4, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n",
      "/home/ubuntu/miniconda3/envs/run/lib/python3.10/site-packages/unsloth/kernels/utils.py:660: UserWarning: An output with one or more elements was resized since it had shape [1, 4, 2560], which does not match the required output shape [4, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n",
      "  out = torch_matmul(X, W, out = out)\n"
     ]
    }
   ],
   "source": [
    "new_outputs = get_answer_batch(tokenizer, model, prompts, num_times_to_repeat=1, apply_template=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.\n",
      "\n",
      "Okay, let's see. The problem is about the glee club and the football team ordering and eating pizzas, and we need to find out how many pizzas are left. Alright, let's break it down step by step.\n",
      "\n",
      "First, the glee club ordered 20 pizzas. They ate 70% of them. So, to find out how many they ate, I need to calculate 70% of 20. Let me write that down. 70% is the same as 0.70 in decimal. So 20 multiplied by 0.70. Let me do that calculation. 20 * 0.7 is 14. So they ate 14 pizzas. That means the number of pizzas left for the glee club is the total ordered minus the ones eaten. So 20 - 14 = 6. So they have 6 left.\n",
      "\n",
      "Now, the football team ordered twice as many pizzas as the glee club. The glee club ordered 20, so twice that is 20 * 2 = 40. So the football team ordered 40 pizzas. Then they ate 80%\n"
     ]
    }
   ],
   "source": [
    "print(new_outputs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "5\n",
      "70\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "for n in new_outputs:\n",
    "    print(parse_llm_answer(n[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "run",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
