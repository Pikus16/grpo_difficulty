{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/run/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 06-05 22:36:14 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 22:36:14,542\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import textstat\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name):\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "\n",
    "    max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "    dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_name,\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    )\n",
    "    if name != 'base':\n",
    "        model.load_adapter(f'models/photosynthesis_{name}')\n",
    "    _ = FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": 250,\n",
    "    \"use_cache\": True,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": None,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "NUM_TIMES_TO_GEN = 100\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "prompt = 'Describe photosynthesis. Use as simple terms as possible'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, calc_entropy=False):\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        [{'role': 'user', 'content': prompt}],\n",
    "        tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        formatted_prompt\n",
    "    ]*BATCH_SIZE, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    all_outputs = []\n",
    "    for _ in tqdm(range(0, NUM_TIMES_TO_GEN, BATCH_SIZE)):\n",
    "        outputs = model.generate(**inputs, **generation_kwargs)\n",
    "        output = tokenizer.batch_decode(outputs)\n",
    "        outputs = outputs[:, inputs.input_ids.shape[1]:]\n",
    "        output = tokenizer.batch_decode(outputs)\n",
    "        all_outputs.extend(output)\n",
    "    return all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(outputs, pad_or_eos, prompt_lens):\n",
    "    # 1) Stack all score‚Äêtensors into a single [T_max, B, V] tensor:\n",
    "    logits_per_step = torch.stack(outputs.scores, dim=0).to(\"cpu\")\n",
    "\n",
    "    # 2) Compute raw entropies at every (step, batch) pair:\n",
    "    probs_per_step = F.softmax(logits_per_step, dim=-1)     # (T_max, B, V)\n",
    "    entropy_per_step = -torch.sum(probs_per_step * torch.log(probs_per_step + 1e-12), dim=-1)\n",
    "    T_max, _ = entropy_per_step.shape\n",
    "\n",
    "    # 3) Find each example‚Äôs ‚Äúnumber of real generated tokens‚Äù G_b:\n",
    "    sequences = outputs.sequences                      # shape = (B, prompt_len + T_max)\n",
    "    B, L_full = sequences.shape\n",
    "    \n",
    "    #   a list of length B, each entry = how many prompt tokens were real (non-pad).\n",
    "\n",
    "    G = []  # will store G_b for each example\n",
    "    for b in range(B):\n",
    "        # We only need to search in the ‚Äúgenerated region‚Äù of that row:\n",
    "        start = prompt_lens[b]\n",
    "        row = sequences[b]\n",
    "        # Look for the first EOS in positions [start, L_full):\n",
    "        sub = row[start:]\n",
    "        eos_positions = (sub == pad_or_eos).nonzero(as_tuple=True)[0]\n",
    "        if len(eos_positions) > 0:\n",
    "            # Suppose the first EOS is at index i_sub within `sub`.\n",
    "            # In the full row, that is at position t_b_full = start + i_sub.\n",
    "            # The number of generated tokens (including EOS) is:\n",
    "            G_b = eos_positions[0].item() + 1\n",
    "        else:\n",
    "            # If EOS never appeared (e.g. we hit max_new_tokens), then\n",
    "            # treat G_b = length of the generated‚Äêregion in the full tensor:\n",
    "            G_b = L_full - start\n",
    "        G.append(G_b)\n",
    "\n",
    "    # 4) Build a boolean mask of shape (T_max, B), True exactly for t < G[b]:\n",
    "    mask = torch.zeros((T_max, B), dtype=torch.bool)\n",
    "    for b in range(B):\n",
    "        real_steps = G[b]              # number of steps that are truly ‚Äúgenerated‚Äù\n",
    "        if real_steps > 0:\n",
    "            mask[:real_steps, b] = True\n",
    "\n",
    "    # 5) Zero‚Äêout any entropy beyond each example‚Äôs G_b:\n",
    "    entropy_real = entropy_per_step * mask.float()  \n",
    "    # shape = (T_max, B).  Rows ‚â• G[b] in column b are now zero.\n",
    "\n",
    "    #    ‚Ä¢ To compute each example‚Äôs average entropy (over just its G_b tokens):\n",
    "    sum_per_example = entropy_real.sum(dim=0)        # (B,)\n",
    "    avg_per_example = sum_per_example / torch.tensor(G, dtype=torch.float)  # (B,)\n",
    "\n",
    "    return avg_per_example.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_entropy(model, tokenizer):\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        [{'role': 'user', 'content': prompt}],\n",
    "        tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        formatted_prompt\n",
    "    ]*BATCH_SIZE, return_tensors = \"pt\").to(\"cuda\")\n",
    "    prompt_lens = (inputs[\"attention_mask\"] == 1).sum(dim=1).tolist()  \n",
    "\n",
    "    all_outputs = []\n",
    "    all_entropies = []\n",
    "    for _ in tqdm(range(0, NUM_TIMES_TO_GEN, BATCH_SIZE)):\n",
    "        outputs = model.generate(**inputs, **generation_kwargs, return_dict_in_generate=True, output_scores=True)\n",
    "        all_entropies.extend(calc_entropy(outputs, tokenizer.eos_token_id, prompt_lens))\n",
    "        output_sequence = outputs.sequences\n",
    "        output_sequence = output_sequence[:, inputs.input_ids.shape[1]:]\n",
    "        output = tokenizer.batch_decode(output_sequence)\n",
    "        all_outputs.extend(output)\n",
    "    return all_outputs, all_entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.9: Fast Llama patching. Transformers: 4.52.3. vLLM: 0.8.2.\n",
      "   \\\\   /|    NVIDIA A10G. Num GPUs = 1. Max memory: 22.069 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "entropy_per_run = []\n",
    "for name in ['base', '0.1_longtrain/lora',  '1_1k_8gen/lora_1k', '1_1k_8gen/lora_2.2k']:\n",
    "    model_, tokenizer_ = load_model(name)\n",
    "    # all_outputs = generate(model_, tokenizer_)\n",
    "    # with open(f'outputs/photosynthesis/{name}.json', 'w') as f:\n",
    "    #     json.dump({'prompt': prompt, 'outputs': all_outputs}, f)\n",
    "    all_outputs_, all_entropies_ = generate_with_entropy(model_, tokenizer_)\n",
    "    entropy_per_run.append(all_outputs_)\n",
    "    print(f'{name}: Entropy - {np.mean(all_entropies_):0.2f} +/- {np.std(all_entropies_):0.2f}')\n",
    "\n",
    "    scores = [textstat.flesch_kincaid_grade(r) for r in all_outputs_]\n",
    "    print(f'Flesch - {np.mean(scores):0.2f} +/- {np.std(scores):0.2f}')\n",
    "    print('--------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "run",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
