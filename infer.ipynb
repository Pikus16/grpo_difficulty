{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/run/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 06-07 16:24:28 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 16:24:29,132\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import textstat\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name):\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "\n",
    "    max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "    dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_name,\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    )\n",
    "    if name != 'base':\n",
    "        model.load_adapter(f'runs/checkpoint-{name}')\n",
    "    _ = FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": 250,\n",
    "    \"use_cache\": True,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": None,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "NUM_TIMES_TO_GEN = 100\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "prompt = 'Describe photosynthesis. Use as simple terms as possible'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(generate_output, tokenizer, input_ids, eps: float = 1e-12):\n",
    "    \"\"\"\n",
    "    Compute the tokenâ€wise entropy for each sequence in a HuggingFace generate() output.\n",
    "\n",
    "    Args:\n",
    "        generate_output (ModelOutput): the output of model.generate(...,\n",
    "            return_dict_in_generate=True, output_scores=True)\n",
    "        eps (float): small constant to avoid log(0)\n",
    "\n",
    "    Returns:\n",
    "        List[List[float]]: entropy per generated token, per batch element.\n",
    "                          entropies[i][j] is the entropy of token j in sequence i.\n",
    "    \"\"\"\n",
    "    # generate_output.scores is a list of length seq_len_generated,\n",
    "    # each element is Tensor(batch_size, vocab_size)\n",
    "    scores = generate_output.scores # List[Tensor(batch, vocab_size)]\n",
    "    sequences = generate_output.sequences  # Tensor(batch, input_len + gen_len)\n",
    "\n",
    "    batch_size = scores[0].size(0)\n",
    "    input_lengths = (input_ids != tokenizer.pad_token_id).sum(dim=1)  # shape: (batch_size,)\n",
    "\n",
    "    entropies = [[] for _ in range(batch_size)]\n",
    "\n",
    "    for step_logits in scores:\n",
    "        # step_logits: (batch_size, vocab_size)\n",
    "        # compute probabilities\n",
    "        probs = F.softmax(step_logits, dim=-1)  # (batch_size, vocab_size)\n",
    "        # compute entropy: -sum p * log p\n",
    "        step_entropy = -torch.sum(probs * torch.log(probs + eps), dim=-1)  # (batch_size,)\n",
    "        # append each batchâ€elementâ€™s entropy for this step\n",
    "        for b in range(batch_size):\n",
    "            entropies[b].append(step_entropy[b].item())\n",
    "\n",
    "    # Now, trim entropies after EOS (or max generated length)\n",
    "    avg_entropies = []\n",
    "    for b in range(batch_size):\n",
    "        seq = sequences[b][input_lengths[b]:]  # only the generated tokens\n",
    "        ent = entropies[b]\n",
    "\n",
    "        if tokenizer.eos_token_id is not None:\n",
    "            # Find index of first EOS token in generation\n",
    "            eos_pos = (seq == tokenizer.eos_token_id).nonzero(as_tuple=True)[0]\n",
    "            if len(eos_pos) > 0:\n",
    "                cutoff = eos_pos[0].item()  # Stop at first EOS\n",
    "                ent = ent[:cutoff]\n",
    "        if len(ent) > 0:\n",
    "            avg_entropies.append(np.mean(ent))\n",
    "\n",
    "    return np.mean(avg_entropies)\n",
    "\n",
    "\n",
    "def generate(model, tokenizer):\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        [{'role': 'user', 'content': prompt}],\n",
    "        tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        formatted_prompt\n",
    "    ]*BATCH_SIZE, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    all_outputs, all_entropies = [], []\n",
    "    for _ in tqdm(range(0, NUM_TIMES_TO_GEN, BATCH_SIZE)):\n",
    "        outputs = model.generate(**inputs, **generation_kwargs, return_dict_in_generate=True, output_scores=True)\n",
    "        all_entropies.append(calculate_entropy(outputs))\n",
    "        outputs = outputs.sequences[:, inputs.input_ids.shape[1]:]\n",
    "        output = tokenizer.batch_decode(outputs)\n",
    "        all_outputs.extend(output)\n",
    "    return all_outputs, np.mean(all_entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(outputs):\n",
    "    entropies = []\n",
    "    for score in outputs.scores:\n",
    "        probs   = F.softmax(score.squeeze(1), dim=-1)  # [1, vocab_size]\n",
    "        entropy = -(probs * (probs + 1e-12).log()).sum(dim=-1)   # [1]\n",
    "        entropies.append(entropy.item())\n",
    "\n",
    "    return np.mean(entropies), np.std(entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_entropy(model, tokenizer):\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        [{'role': 'user', 'content': prompt}],\n",
    "        tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(**inputs, **generation_kwargs, return_dict_in_generate=True, output_scores=True)\n",
    "    entropy_stats = calc_entropy(outputs)\n",
    "    output_sequence = outputs.sequences\n",
    "    output_sequence = output_sequence[:, inputs.input_ids.shape[1]:]\n",
    "    output = tokenizer.batch_decode(output_sequence)[0]\n",
    "    return output, entropy_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5269858230784722 967\n",
      "0.030398263298359113 979\n",
      "0.06187368127015921 998\n",
      "0.11231327801942825 1193\n",
      "0.09775299673349085 1122\n",
      "0.1278582625918918 732\n",
      "0.2503827397823334 652\n",
      "0.2420045623412499 1031\n",
      "0.2257803423660938 1106\n",
      "0.3075013137133108 958\n",
      "0.29372991709148183 947\n"
     ]
    }
   ],
   "source": [
    "# for temperature in np.arange(0, 1.01, 0.1):\n",
    "#     formatted_prompt = tokenizer_.apply_chat_template(\n",
    "#         [{'role': 'user', 'content': prompt}],\n",
    "#         tokenize=False, add_generation_prompt=True)\n",
    "#     inputs = tokenizer_(formatted_prompt, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "#     if temperature == 0:\n",
    "#         do_sample = False\n",
    "#     else:\n",
    "#         do_sample = True\n",
    "#     generation_kwargs = {\n",
    "#         \"max_new_tokens\": 250,\n",
    "#         \"use_cache\": True,\n",
    "#         \"temperature\": temperature,\n",
    "#         \"top_k\": None,\n",
    "#         \"do_sample\": do_sample,\n",
    "#     }\n",
    "\n",
    "#     gen_output = model_.generate(\n",
    "#         **inputs,\n",
    "#         **generation_kwargs,\n",
    "#         output_scores=True,\n",
    "#         return_dict_in_generate=True,\n",
    "#     )\n",
    "\n",
    "#     entropies = []\n",
    "#     import torch.nn.functional as F\n",
    "#     for score in gen_output.scores:\n",
    "#         probs   = F.softmax(score.squeeze(1), dim=-1)  # [1, vocab_size]\n",
    "#         entropy = -(probs * (probs + 1e-12).log()).sum(dim=-1)   # [1]\n",
    "#         entropies.append(entropy.item())\n",
    "\n",
    "#     generated_ids   = gen_output.sequences       # [1, prompt_len + max_new_tokens]\n",
    "#     generated_text  = tokenizer_.decode(generated_ids[0], skip_special_tokens=True)\n",
    "#     print(np.mean(entropies), len(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.1: Fast Llama patching. Transformers: 4.52.4. vLLM: 0.8.2.\n",
      "   \\\\   /|    NVIDIA A10G. Num GPUs = 1. Max memory: 22.069 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "calc_entropies = []\n",
    "for name in ['600','800','1000','1200'] + os.listdir('models/photosynthesis/'):\n",
    "    model_, tokenizer_ = load_model(name)\n",
    "    all_outputs, mean_entropy = generate(model_, tokenizer_)\n",
    "    with open(f'outputs/photosynthesis/0.1_score4.11_numgen4/{name}.json', 'w') as f:\n",
    "        json.dump({'prompt': prompt, 'outputs': all_outputs}, f)\n",
    "    calc_entropies.append(mean_entropy)\n",
    "    print(mean_entropy)\n",
    "    # output_str, entropy_stats = generate_with_entropy(model_, tokenizer_)\n",
    "    # print(name)\n",
    "    # print(f'Entropy - {entropy_stats[0]:0.2f} +/- {entropy_stats[1]:0.2f}')\n",
    "\n",
    "    # score = textstat.flesch_kincaid_grade(output_str)\n",
    "    # print(f'Flesch - {score}')\n",
    "    # print('--------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "run",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
