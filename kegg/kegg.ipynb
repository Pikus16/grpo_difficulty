{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 06-27 17:10:01 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from kegg_utils import load_test_model_and_tokenizer, load_kegg_dataset, parse_llm_responses\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.1: Fast Llama patching. Transformers: 4.52.4. vLLM: 0.8.2.\n",
      "   \\\\   /|    NVIDIA A10G. Num GPUs = 1. Max memory: 22.069 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/Meta-Llama-3.1-8B-Instruct with actual GPU utilization = 88.82%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 22.07 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4000. Num Sequences = 192.\n",
      "Unsloth: vLLM's KV Cache can use up to 4.57 GB. Also swap space = 0 GB.\n",
      "INFO 06-27 17:10:14 [config.py:585] This model supports multiple tasks: {'score', 'classify', 'embed', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 06-27 17:10:14 [arg_utils.py:1865] LORA is experimental on VLLM_USE_V1=1. Falling back to V0 Engine.\n",
      "INFO 06-27 17:10:14 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.2) with config: model='unsloth/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='unsloth/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/Meta-Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"backend\":\"inductor\",\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"debug\":false,\"dce\":true,\"coordinate_descent_tuning\":true,\"trace.enabled\":false,\"trace.graph_diagram\":false,\"triton.cudagraphs\":true,\"compile_threads\":48,\"max_autotune\":false,\"disable_progress\":false,\"verbose_progress\":true,\"enable_auto_functionalized_v2\":false},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":192}, use_cached_outputs=False, \n",
      "INFO 06-27 17:10:15 [cuda.py:291] Using Flash Attention backend.\n",
      "INFO 06-27 17:10:15 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 06-27 17:10:15 [model_runner.py:1110] Starting to load model unsloth/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 06-27 17:10:16 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8013139673b7402db1498cc14a617ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = load_test_model_and_tokenizer(adapter_path='/home/ubuntu/code/grpo_difficulty/kegg/models/kegg_correctness/lora',\n",
    "                                            max_seq_length=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_kegg_dataset(split='test')\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses(tokenizer, model,prompts, num_times_to_repeat: int = 1):\n",
    "    generation_kwargs = {\n",
    "        \"max_new_tokens\": 600,\n",
    "        \"use_cache\": True,\n",
    "        \"temperature\": 0.9,\n",
    "        \"top_k\": None,\n",
    "        \"do_sample\": True,\n",
    "    }\n",
    "\n",
    "     # Create all formatted prompts at once\n",
    "    all_formatted_prompts = []\n",
    "    for prompt in prompts:\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            [{'role': 'user', 'content': prompt}],\n",
    "            tokenize=False, add_generation_prompt=True)\n",
    "        all_formatted_prompts.extend([formatted_prompt] * num_times_to_repeat)\n",
    "\n",
    "    # Tokenize in larger batches\n",
    "    inputs = tokenizer(all_formatted_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        outputs = model.generate(**inputs, **generation_kwargs)\n",
    "    \n",
    "    outputs = outputs[:, inputs.input_ids.shape[1]:]\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "    # Reshape outputs back to per-prompt format\n",
    "    result = []\n",
    "    for i in range(len(prompts)):\n",
    "        start_idx = i * num_times_to_repeat\n",
    "        end_idx = start_idx + num_times_to_repeat\n",
    "        output = decoded_outputs[start_idx:end_idx]\n",
    "        if num_times_to_repeat == 1:\n",
    "            output = output[0]\n",
    "        result.append(output)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [21:26<00:00, 34.77s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "all_outputs = []\n",
    "for i in tqdm(range(0, len(ds), batch_size), desc=\"Processing batches\"):\n",
    "\n",
    "    batch_end = min(i + batch_size, len(ds))\n",
    "    batch = ds[i:batch_end]\n",
    "    \n",
    "    \n",
    "    if batch_size == 1 or batch_end - i == 1:\n",
    "        qs = [batch['text']]\n",
    "        answers = [batch['answer']]\n",
    "    else:\n",
    "        qs = batch['text']\n",
    "        answers = batch['answer']\n",
    "    batch_outputs = get_responses(tokenizer, model, qs, num_times_to_repeat=1)\n",
    "    all_outputs.extend(batch_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [ds[i]['answer'] for i in range(len(all_outputs))]\n",
    "obj_to_dump = {'responses' : all_outputs, 'answer' : answers}\n",
    "with open('outputs/grpo_correctness_test_1000.json', 'w') as f:\n",
    "    json.dump(obj_to_dump, f, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146, 146)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [parse_llm_responses(x) for x in all_outputs]\n",
    "len(predictions), len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact: 49, Has Answer: 17, Error Process: 22, Wrong: 58, Total: 146\n"
     ]
    }
   ],
   "source": [
    "ct_exact, ct_in, ct_none, ct_wrong = 0,0,0, 0\n",
    "for pred, ans in zip(predictions, answers):\n",
    "    if pred is None:\n",
    "        ct_none += 1\n",
    "    else:\n",
    "        if pred == ans:\n",
    "            ct_exact += 1\n",
    "        elif ans in pred:\n",
    "            ct_in += 1\n",
    "        else:\n",
    "            ct_wrong += 1\n",
    "print(f'Exact: {ct_exact}, Has Answer: {ct_in}, Error Process: {ct_none}, Wrong: {ct_wrong}, Total: {len(predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4520547945205479"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "model, tokenizer = load_test_model_and_tokenizer(adapter_path='/home/ubuntu/code/grpo_difficulty/kegg/runs/checkpoint-400',\n",
    "                                            max_seq_length=4000)\n",
    "\n",
    "all_outputs = []\n",
    "for i in tqdm(range(0, len(ds), batch_size), desc=\"Processing batches\"):\n",
    "\n",
    "    batch_end = min(i + batch_size, len(ds))\n",
    "    batch = ds[i:batch_end]\n",
    "    \n",
    "    \n",
    "    if batch_size == 1 or batch_end - i == 1:\n",
    "        qs = [batch['text']]\n",
    "        answers = [batch['answer']]\n",
    "    else:\n",
    "        qs = batch['text']\n",
    "        answers = batch['answer']\n",
    "    batch_outputs = get_responses(tokenizer, model, qs, num_times_to_repeat=1)\n",
    "    all_outputs.extend(batch_outputs)\n",
    "\n",
    "answers = [ds[i]['answer'] for i in range(len(all_outputs))]\n",
    "obj_to_dump = {'responses' : all_outputs, 'answer' : answers}\n",
    "with open('outputs/grpo_correctness_test_400.json', 'w') as f:\n",
    "    json.dump(obj_to_dump, f, )\n",
    "\n",
    "predictions = [parse_llm_responses(x) for x in all_outputs]\n",
    "len(predictions), len(answers)\n",
    "\n",
    "ct_exact, ct_in, ct_none, ct_wrong = 0,0,0, 0\n",
    "for pred, ans in zip(predictions, answers):\n",
    "    if pred is None:\n",
    "        ct_none += 1\n",
    "    else:\n",
    "        if pred == ans:\n",
    "            ct_exact += 1\n",
    "        elif ans in pred:\n",
    "            ct_in += 1\n",
    "        else:\n",
    "            ct_wrong += 1\n",
    "print(f'Exact: {ct_exact}, Has Answer: {ct_in}, Error Process: {ct_none}, Wrong: {ct_wrong}, Total: {len(predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "run",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
