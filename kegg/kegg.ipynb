{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 06-26 22:11:22 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from kegg_utils import load_model_and_tokenizer, load_kegg_dataset, parse_llm_responses\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(max_seq_length=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_kegg_dataset(split='test')\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses(tokenizer, model,prompts, num_times_to_repeat: int = 1):\n",
    "    generation_kwargs = {\n",
    "        \"max_new_tokens\": 600,\n",
    "        \"use_cache\": True,\n",
    "        \"temperature\": 0.9,\n",
    "        \"top_k\": None,\n",
    "        \"do_sample\": True,\n",
    "    }\n",
    "\n",
    "     # Create all formatted prompts at once\n",
    "    all_formatted_prompts = []\n",
    "    for prompt in prompts:\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            [{'role': 'user', 'content': prompt}],\n",
    "            tokenize=False, add_generation_prompt=True)\n",
    "        all_formatted_prompts.extend([formatted_prompt] * num_times_to_repeat)\n",
    "\n",
    "    # Tokenize in larger batches\n",
    "    inputs = tokenizer(all_formatted_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        outputs = model.generate(**inputs, **generation_kwargs)\n",
    "    \n",
    "    outputs = outputs[:, inputs.input_ids.shape[1]:]\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "    # Reshape outputs back to per-prompt format\n",
    "    result = []\n",
    "    for i in range(len(prompts)):\n",
    "        start_idx = i * num_times_to_repeat\n",
    "        end_idx = start_idx + num_times_to_repeat\n",
    "        result.append(decoded_outputs[start_idx:end_idx])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [21:26<00:00, 34.77s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "all_outputs = []\n",
    "for i in tqdm(range(0, len(ds), batch_size), desc=\"Processing batches\"):\n",
    "\n",
    "    batch_end = min(i + batch_size, len(ds))\n",
    "    batch = ds[i:batch_end]\n",
    "    \n",
    "    \n",
    "    if batch_size == 1 or batch_end - i == 1:\n",
    "        qs = [batch['text']]\n",
    "        answers = [batch['answer']]\n",
    "    else:\n",
    "        qs = batch['text']\n",
    "        answers = batch['answer']\n",
    "    batch_outputs = get_responses(tokenizer, model, qs, num_times_to_repeat=1)\n",
    "    all_outputs.extend(batch_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_outputs = []\n",
    "for a in all_outputs:\n",
    "    assert isinstance(a, list)\n",
    "    assert len(a) == 1\n",
    "    new_outputs.append(a[0])\n",
    "all_outputs = new_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [ds[i]['answer'] for i in range(len(all_outputs))]\n",
    "obj_to_dump = {'responses' : all_outputs, 'answer' : answers}\n",
    "with open('outputs/kegg_test.json', 'w') as f:\n",
    "    json.dump(obj_to_dump, f, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146, 146)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [parse_llm_responses(x) for x in all_outputs]\n",
    "len(predictions), len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact: 49, Has Answer: 17, Error Process: 22, Wrong: 58, Total: 146\n"
     ]
    }
   ],
   "source": [
    "ct_exact, ct_in, ct_none, ct_wrong = 0,0,0, 0\n",
    "for pred, ans in zip(predictions, answers):\n",
    "    if pred is None:\n",
    "        ct_none += 1\n",
    "    else:\n",
    "        if pred == ans:\n",
    "            ct_exact += 1\n",
    "        elif ans in pred:\n",
    "            ct_in += 1\n",
    "        else:\n",
    "            ct_wrong += 1\n",
    "print(f'Exact: {ct_exact}, Has Answer: {ct_in}, Error Process: {ct_none}, Wrong: {ct_wrong}, Total: {len(predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4520547945205479"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(49 + 17) / 146"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "run",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
